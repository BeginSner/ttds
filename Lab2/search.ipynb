{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from nltk.stem import *\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load index into memory\n",
    "with open('collections/trec.english_stemmer.dat', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "len(data)\n",
    "len(data['incom'].keys() & data['come'].keys() & data['like'].keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_search(input, data):\n",
    "    doc_dic = {}\n",
    "    reg_bra = re.compile(r\"^#\\d{1,}\\((.*?)[)]\") # begin with # and at least 1 number and in ()\n",
    "    reg_num = re.compile(r\"\\d{1,}\") # distance\n",
    "    input = input.lower()\n",
    "    sentence = reg_bra.findall(input)[0]\n",
    "    distance = int(reg_num.findall(input)[0])\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    punct = re.compile(\"[^\\w\\s]\") # regax of token\n",
    "    sentence = re.sub(punct, \"\", sentence) # remove ,\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    sentence = sentence.split()\n",
    "    sentence = [stemmer.stem(word) if not word == stemmer.stem(word) else word for word in sentence]\n",
    "    public_key = data[sentence[0]].keys() & data[sentence[1]].keys() # find common keys for two words\n",
    "    for key in public_key:\n",
    "        for index0 in data[sentence[0]][key]:\n",
    "            for index1 in data[sentence[1]][key]:\n",
    "                if np.abs(index0-index1) < distance:\n",
    "                    doc_dic[key] = [index0, index1]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return doc_dic, sentence\n",
    "content = '#10(income, taxes)'\n",
    "# doc_dic, _ = proximity_search(content, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3706': [189, 190],\n",
       " '3449': [685, 686],\n",
       " '3699': [64, 67],\n",
       " '3818': [96, 97],\n",
       " '282': [170, 171],\n",
       " '361': [231, 232],\n",
       " '3708': [93, 94],\n",
       " '3599': [46, 49],\n",
       " '3817': [20, 23],\n",
       " '3405': [507, 508],\n",
       " '3562': [157, 158],\n",
       " '3734': [278, 279],\n",
       " '65': [353, 354],\n",
       " '92': [203, 204],\n",
       " '3441': [427, 428]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def phrase_search(input, data):\n",
    "    doc_dic = {}\n",
    "    reg_quo = re.compile(r'\"(.*?)\"') # in \"\"\n",
    "    input = input.lower()\n",
    "    sentence = reg_quo.findall(input)[0]\n",
    "    distance = 5\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    punct = re.compile(\"[^\\w\\s]\") # regax of token\n",
    "    sentence = re.sub(punct, \"\", sentence) # remove ,\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    sentence = sentence.split()\n",
    "    sentence = [stemmer.stem(word) if not word == stemmer.stem(word) else word for word in sentence]\n",
    "    public_key = data[sentence[0]].keys() & data[sentence[1]].keys() # find common keys for two words\n",
    "    for key in public_key:\n",
    "        for index0 in data[sentence[0]][key]:\n",
    "            for index1 in data[sentence[1]][key]:\n",
    "                if np.abs(index0-index1) < distance:\n",
    "                    doc_dic[key] = [index0, index1]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return doc_dic, sentence\n",
    "content = '\"income taxes\"'\n",
    "doc_dic, _ = phrase_search(content, data)\n",
    "doc_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolearn_search(input, data, reg=None, label:'str'=None):\n",
    "    input = input.lower()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    reg_quo = re.compile(r'\"(.*?)\"') # in \"\"\n",
    "    reg_bra = re.compile(r\"^#\\d{1,}\\((.*?)[)]\") # begin with # and at least 1 number and in ()\n",
    "    punct = re.compile(\"[^\\w\\s]\") # regax of token\n",
    "    if reg == None:\n",
    "        reg = reg_quo\n",
    "\n",
    "    if label == None:\n",
    "        if not reg.findall(input):\n",
    "            sentence = re.sub(punct, \"\", input) # remove ,\n",
    "            sentence = remove_stopwords(sentence)\n",
    "            sentence = sentence.split()\n",
    "            sentence = [stemmer.stem(word) if not word == stemmer.stem(word) else word for word in sentence]\n",
    "            public_key = data[sentence[0]].keys()\n",
    "            for term in sentence:\n",
    "                public_key = data[term].keys() & public_key\n",
    "            sentence = ' '.join(sentence)\n",
    "            return public_key, sentence\n",
    "        elif reg.findall(input):\n",
    "            if reg == reg_quo:\n",
    "                return phrase_search(input, data)\n",
    "            else: \n",
    "                return proximity_search(input, data)\n",
    "            \n",
    "        \n",
    "    elif label == 'and':\n",
    "        split_list = input.split(\" and \")\n",
    "    elif label == 'or':\n",
    "        split_list = input.split(\" or \")\n",
    "    elif label == 'and not':\n",
    "        split_list = input.split(\" and not \")\n",
    "    else: \n",
    "        print(\"error label!\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not reg.findall(split_list[0]) and not reg.findall(split_list[1]):\n",
    "        sentence1 = re.sub(punct, \"\", split_list[0]) # remove ,\n",
    "        sentence1 = remove_stopwords(sentence1)\n",
    "        sentence1 = sentence1.split()\n",
    "        sentence1 = [stemmer.stem(word) if not word == stemmer.stem(word) else word for word in sentence1]\n",
    "        public_key1 = data[sentence1[0]].keys()\n",
    "        for term in sentence1:\n",
    "            public_key1 = data[term].keys() & public_key1\n",
    "        sentence1 = ' '.join(sentence1)\n",
    "\n",
    "        sentence2 = re.sub(punct, \"\", split_list[1]) # remove ,\n",
    "        sentence2 = remove_stopwords(sentence2)\n",
    "        sentence2 = sentence2.split()\n",
    "        sentence2 = [stemmer.stem(word) if not word == stemmer.stem(word) else word for word in sentence2]\n",
    "        public_key2 = data[sentence2[0]].keys()\n",
    "        for term in sentence2:\n",
    "            public_key2 = data[term].keys() & public_key2\n",
    "        sentence2 = ' '.join(sentence2)\n",
    "        \n",
    "        if label == 'and not':\n",
    "            public_key = public_key1 - public_key2\n",
    "            sentence = sentence1\n",
    "        elif label == 'and':\n",
    "            public_key = public_key1 & public_key2\n",
    "            sentence = sentence1 + ' ' + sentence2\n",
    "        elif label == 'or':\n",
    "            public_key = public_key1 | public_key2\n",
    "            sentence = sentence1 + ' ' + sentence2\n",
    "        return public_key, sentence1\n",
    "\n",
    "    elif reg.findall(split_list[0]) and reg.findall(split_list[1]):\n",
    "        if reg == reg_quo:\n",
    "            doc_dic1, sentence1 = phrase_search(split_list[0], data)\n",
    "            doc_dic2, sentence2 = phrase_search(split_list[1], data)\n",
    "        else: \n",
    "            doc_dic1, sentence1 = proximity_search(split_list[0], data)\n",
    "            doc_dic2, sentence2 = proximity_search(split_list[1], data)\n",
    "        if label == 'and not':\n",
    "            public_key = doc_dic1.keys() - doc_dic2.keys()\n",
    "            sentence = sentence1\n",
    "        elif label == 'and':\n",
    "            public_key = doc_dic1.keys() & doc_dic2.keys()\n",
    "            sentence = sentence1 + ' ' + sentence2\n",
    "        elif label == 'or':\n",
    "            public_key = doc_dic1.keys() | doc_dic2.keys()\n",
    "            sentence = sentence1 + ' ' + sentence2\n",
    "        return public_key, sentence\n",
    "\n",
    "    elif not reg.findall(split_list[0]) and reg.findall(split_list[1]):\n",
    "        sentence = re.sub(punct, \"\", split_list[0]) # remove ,\n",
    "        sentence = remove_stopwords(sentence)\n",
    "        sentence = sentence.split()\n",
    "        sentence = [stemmer.stem(word) if not word == stemmer.stem(word) else word for word in sentence]\n",
    "        public_key = data[sentence[0]].keys()\n",
    "        for term in sentence:\n",
    "            public_key = data[term].keys() & public_key\n",
    "        sentence = ' '.join(sentence)\n",
    "\n",
    "        if reg == reg_quo:\n",
    "            doc_dic2, sentence2 = phrase_search(split_list[1], data)\n",
    "        else:\n",
    "            doc_dic2, sentence2 = proximity_search(split_list[1], data)\n",
    "        if label == 'and not':\n",
    "            public_key = public_key1 - doc_dic2.keys()\n",
    "            sentence = sentence\n",
    "        elif label == 'and':\n",
    "            public_key = public_key1 & doc_dic2.keys()\n",
    "            sentence = sentence + ' ' + sentence2\n",
    "        elif label == 'or':\n",
    "            public_key = public_key1 | doc_dic2.keys()\n",
    "            sentence = sentence + ' ' + sentence2\n",
    "        return public_key, sentence\n",
    "\n",
    "    elif reg.findall(split_list[0]) and not reg.findall(split_list[1]):\n",
    "        if reg == reg_quo:\n",
    "            doc_dic1, sentence1 = phrase_search(split_list[0], data)\n",
    "        else: \n",
    "            doc_dic1, sentence1 = proximity_search(split_list[0], data)\n",
    "        sentence = re.sub(punct, \"\", split_list[1]) # remove ,\n",
    "        sentence = remove_stopwords(sentence)\n",
    "        sentence = sentence.split()\n",
    "        sentence = [stemmer.stem(word) if not word == stemmer.stem(word) else word for word in sentence]\n",
    "        public_key = data[sentence[0]].keys()\n",
    "        for term in sentence:\n",
    "            public_key = data[term].keys() & public_key\n",
    "        sentence = ' '.join(sentence)\n",
    "\n",
    "        \n",
    "        if label == 'and not':\n",
    "            public_key = doc_dic1.keys() - public_key\n",
    "            sentence = sentence1\n",
    "        elif label == 'and':\n",
    "            public_key = doc_dic1.keys() & public_key\n",
    "            sentence = sentence1 + ' ' + sentence\n",
    "        elif label == 'or':\n",
    "            public_key = doc_dic1.keys() | public_key\n",
    "            sentence = sentence1 + ' ' + sentence\n",
    "        return public_key, sentence\n",
    "        \n",
    "    else: \n",
    "        print(\"error\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'219', '223', '288', '305', '3549', '3663', '3762', '3766'},\n",
       " 'middl east peac')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(input, data):\n",
    "    input = input.lower()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    punct = re.compile(\"[^\\w\\s]\") # regax of token\n",
    "    reg_quo = re.compile(r'\"(.*?)\"') # in \"\"\n",
    "    reg_bra = re.compile(r\"^#\\d{1,}\\((.*?)[)]\") # begin with # and at least 1 number and in ()\n",
    "    # reg_num = re.compile(r\"\\d{1,}\") # distance\n",
    "    if ' and not ' in input:\n",
    "        if reg_quo.findall(input):\n",
    "            return boolearn_search(input, data, reg_quo, label='and not')\n",
    "        elif reg_bra.findall(input):\n",
    "            return boolearn_search(input, data, reg_bra, label='and not')\n",
    "        else:\n",
    "            return boolearn_search(input, data, label='and not')\n",
    "    elif ' and ' in input:\n",
    "        if reg_quo.findall(input):\n",
    "            return boolearn_search(input, data, reg_quo, label='and')\n",
    "        elif reg_bra.findall(input):\n",
    "            return boolearn_search(input, data, reg_bra, label='and')\n",
    "        else:\n",
    "            return boolearn_search(input, data, label='and')\n",
    "    if ' or ' in input:\n",
    "        if reg_quo.findall(input):\n",
    "            return boolearn_search(input, data, reg_quo, label='or')\n",
    "        elif reg_bra.findall(input):\n",
    "            return boolearn_search(input, data, reg_bra, label='or')\n",
    "        else:\n",
    "            return boolearn_search(input, data, label='or')\n",
    "    else:\n",
    "        if reg_quo.findall(input):\n",
    "            return boolearn_search(input, data, reg_quo)\n",
    "        elif reg_bra.findall(input):\n",
    "            return boolearn_search(input, data, reg_bra)\n",
    "        else:\n",
    "            return boolearn_search(input, data)\n",
    "        \n",
    "    \n",
    "        \n",
    "content = '\"middle east\" AND peace'\n",
    "public_key = search(content, data)\n",
    "public_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banana'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_unit = \"I like eating apples and the same with him\"\n",
    "stemmer = SnowballStemmer(\"porter\")\n",
    "sentence_unit = sentence_unit.split()\n",
    "sentence_unit = [stemmer.stem(word) if not word == stemmer.stem(word) else word for word in sentence_unit]\n",
    "sentence_unit = ' '.join(sentence_unit)\n",
    "stemmer.stem('bananas')\n",
    "# boolearn_search(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['middle east']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = re.compile(r'\"(.*?)\"')\n",
    "content = '\"middle east\" AND peace'\n",
    "result = reg.findall(content)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['income, taxes,12']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = re.compile(r\"^#\\d{1,}\\((.*?)[)]\")\n",
    "reg2 = re.compile(r\"\\d{1,}\")\n",
    "content = '#10(income, taxes,12)55'\n",
    "reg.findall(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic1 = {'a':12,'b':'asd'}\n",
    "dic2 = {'a':142, 'c':231}\n",
    "dic1.keys()-dic2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple like hello world'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = ['apple', 'like']\n",
    "l2 = ['hello', 'world']\n",
    "' '.join(l1)+' ' + ' '.join(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53a1c45876d37b93742cefcbccd0e028309dbdecc6e457f55650f19fbe3f8ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
